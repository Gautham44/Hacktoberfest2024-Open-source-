import cv2
import dlib
import numpy as np
import pyautogui

# Load the pre-trained model for facial landmarks
PREDICTOR_PATH = 'shape_predictor_68_face_landmarks.dat'  # Update this path to where you saved the model

# Initialize dlib's face detector and facial landmark predictor
detector = dlib.get_frontal_face_detector()
predictor = dlib.shape_predictor(PREDICTOR_PATH)

# Define the screen dimensions
screen_width, screen_height = pyautogui.size()

# Start capturing video from the webcam
cap = cv2.VideoCapture(0)

while True:
    ret, frame = cap.read()
    if not ret:
        break

    # Convert the frame to grayscale for better performance
    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

    # Detect faces in the frame
    faces = detector(gray)

    for face in faces:
        landmarks = predictor(gray, face)

        # Get the coordinates of the left and right eyes
        left_eye = landmarks.part(36), landmarks.part(37), landmarks.part(38), landmarks.part(39), landmarks.part(40), landmarks.part(41)
        right_eye = landmarks.part(42), landmarks.part(43), landmarks.part(44), landmarks.part(45), landmarks.part(46), landmarks.part(47)

        # Calculate the center of the eyes
        left_eye_center = np.mean(np.array([[p.x, p.y] for p in left_eye]), axis=0)
        right_eye_center = np.mean(np.array([[p.x, p.y] for p in right_eye]), axis=0)

        # Calculate the midpoint between the eyes
        eye_center = ((left_eye_center[0] + right_eye_center[0]) // 2, (left_eye_center[1] + right_eye_center[1]) // 2)

        # Map the eye center to the screen dimensions
        x = np.interp(eye_center[0], [0, frame.shape[1]], [0, screen_width])
        y = np.interp(eye_center[1], [0, frame.shape[0]], [0, screen_height])

        # Move the mouse cursor
        pyautogui.moveTo(x, y)

        # Draw the detected face and eyes on the frame
        cv2.circle(frame, (int(left_eye_center[0]), int(left_eye_center[1])), 5, (0, 255, 0), -1)
        cv2.circle(frame, (int(right_eye_center[0]), int(right_eye_center[1])), 5, (0, 255, 0), -1)

    # Display the frame with detected features
    cv2.imshow("Eye Tracking", frame)

    # Exit if the user presses 'q'
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

# Release the webcam and close the window
cap.release()
cv2.destroyAllWindows()
